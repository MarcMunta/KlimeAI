# RTX 4080 16GB: ruta recomendada

<!-- markdownlint-disable MD013 -->

## Perfil “120B-like” (expert bank)

Perfil recomendado para “120B-like”: `rtx4080_16gb_120b_like`.
Guía completa: `docs/120b_like.md`.

Self-train HF en Windows (WSL2): `docs/WINDOWS_SELF_TRAIN_WSL.md`.

```bash
python -m vortex doctor --deep --mock --profile rtx4080_16gb_120b_like
python -m vortex bench  --mock --profile rtx4080_16gb_120b_like --scenario long_prefill --max-new-tokens 16
python -m vortex prepare-model   --profile rtx4080_16gb_120b_like
python -m vortex bench --profile rtx4080_16gb_120b_like --update-baseline   # baseline (real)
python -m vortex serve        --profile rtx4080_16gb_120b_like
```

### Bench suite (escenarios)

Si el perfil define `bench.scenarios`, puedes ejecutar todo el suite:

```bash
python -m vortex bench --mock --profile rtx4080_16gb_120b_like --suite --json-out data\\bench\\suite_mock.json
```

## Install

```bash
python -m venv .venv
.venv\\Scripts\\activate
python -m pip install -e .
python -m pip install -e ".[api,hf,experts]"
# Opcional (según entorno):
python -m pip install -e ".[train,llama_cpp]"
```

## KV compression (opcional, “lowrank”)

Para el backend core (`vortex`), puedes activar compresión de KV inspirada en MLA:

- `runtime.kv_quant: lowrank`
- `runtime.kv_lowrank_rank: 0` (auto) o un entero (p.ej. `64`)

Mide impacto con `--scenario long_prefill` (prefill largo / decode corto).

## Shared expert (opcional, siempre activo)

Para añadir un “shared expert” que se aplica además del `top_k`:

- `experts.shared_expert_path: <ruta al adapter>`
- `experts.shared_expert_name: shared_expert` (opcional)
- `experts.shared_expert_weight: 0.2` (opcional; se normaliza con el resto)

El bench exporta telemetría agregada (`shared_expert`, `shared_used`, `adapter_cache_hit`, `adapter_load_ms`).

## External engine (opcional: sglang/vLLM proxy)

Perfil: `rtx4080_16gb_sglang_fast` (backend `external`).

- Requiere endpoint OpenAI-compatible en `core.external_base_url` (por defecto `http://127.0.0.1:30000`).
- `doctor --deep` falla “fail-closed” si `sglang`/`vllm` no está instalado (en Windows, preferir WSL2).

Bench con override de engine:

```bash
python -m vortex bench --mock --profile rtx4080_16gb_120b_like --engine sglang
```

## Doctor (sin bajar pesos HF)

```bash
python -m vortex doctor --profile rtx4080_16gb_vortexx --deep
python -m vortex doctor --profile qwen8b_base --deep
```

## Chat / Serve

```bash
python -m vortex chat  --profile rtx4080_16gb_vortexx
python -m vortex serve --profile rtx4080_16gb_vortexx

python -m vortex chat  --profile qwen8b_base
python -m vortex serve --profile qwen8b_base
```

## Bench (unificado)

```bash
python -m vortex bench --profile rtx4080_16gb_vortexx --max-new-tokens 64
python -m vortex bench --profile qwen8b_base --max-new-tokens 64
```

## Self-train (serve-self-train)

HF (QLoRA):

```bash
python -m vortex serve-self-train --profile rtx4080_16gb_safe_hf --host 0.0.0.0 --port 8000
```

Core (Vortex):

```bash
python -m vortex serve-self-train --profile safe_selftrain_4080 --host 0.0.0.0 --port 8000
```

## Autopilot (internet + autopatch seguro)

Perfil recomendado (gated por approval file):

```bash
python -m vortex serve-autopilot --profile autonomous_4080_hf --host 0.0.0.0 --port 8000
```

Para permitir aplicar/mergear parches (por defecto **NO** aplica nada):

```bash
type NUL > data\\APPROVE_AUTOPATCH
```

Runner con rollback automático (si autopilot pide restart o falla):

```bash
python scripts\\run_daemon.py --profile autonomous_4080_hf --host 0.0.0.0 --port 8000
```
